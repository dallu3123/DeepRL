import gymnasium as gym
from lib import dqn_model
from lib import wrappers

from dataclasses import dataclass
import argparse
import time
import numpy as np
import collections
import typing as tt

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.tensorboard.writer import SummaryWriter


DEFAULT_ENV_NAME = "PongNoFrameskip-v4"
MEAN_REWARD_BOUND = 19


# í•˜ì´í¼íŒŒë¼ë¯¸í„°
GAMMA = 0.99 # í• ì¸ ê³„ìˆ˜
BATCH_SIZE = 32 # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
REPLAY_SIZE = 10000 # ë¦¬í”Œë ˆì´ ë²„í¼ í¬ê¸°
LEARNING_RATE = 1e-4 # í•™ìŠµë¥ 
SYNC_TARGET_FRAMES = 1000 # ëª©í‘œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°
REPLAY_START_SIZE = 10000 # ë¦¬í”Œë ˆì´ ì‹œì‘ í¬ê¸°

EPSILON_DECAY_LAST_FRAME = 150000 # íƒí—˜ ê°ì†Œ ìµœì¢… í”„ë ˆì„
EPSILON_START = 1.0 # íƒí—˜ ê°ì†Œ ì‹œì‘ í”„ë ˆì„
EPSILON_FINAL = 0.01 # íƒí—˜ ê°ì†Œ ìµœì¢… í”„ë ˆì„

# ìƒíƒœ, ì•¡ì…˜, ë°°ì¹˜ í…ì„œ íƒ€ì…
State = np.ndarray
Action = int
BatchTensors = tt.Tuple[
    torch.ByteTensor,           # current state
    torch.LongTensor,           # actions
    torch.Tensor,               # rewards
    torch.BoolTensor,           # done || trunc
    torch.ByteTensor            # next state
]

# ê²½í—˜ í´ë˜ìŠ¤
@dataclass
class Experience:
    state: State
    action: Action
    reward: float
    done_or_trunc: bool
    next_state: State

# ë¦¬í”Œë ˆì´ ë²„í¼ í´ë˜ìŠ¤
class ReplayBuffer:
    def __init__(self, capacity: int):
        self.buffer = collections.deque(maxlen=capacity)

    def __len__(self):
        return len(self.buffer)

    def append(self, experience: Experience):
        self.buffer.append(experience)

    # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§
    def sample(self, batch_size: int) -> tt.List[Experience]:
        indices = np.random.choice(len(self), size=batch_size, replace=False)
        return [self.buffer[i] for i in indices]


class Agent:
    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer):
        self.env = env
        self.replay_buffer = replay_buffer
        self.state: tt.Optional[np.ndarray] = None
        self._reset()

    # í™˜ê²½ ë° reward ì´ˆê¸°í™”
    def _reset(self):
        self.state, _ = self.env.reset()
        self.total_reward = 0.0

    @torch.no_grad()
    def play_step(
        self,
        net: dqn_model.DQN,
        device: torch.device,
        epsilon: float = 0.0,
        ) -> tt.Optional[float]:
        '''
        í™˜ê²½ì—ì„œ í•œ ìŠ¤í…ì„ ì‹¤í–‰í•˜ê³  ë³´ìƒì„ ë°˜í™˜í•œë‹¤.
        '''

        done_reward = None

        if np.random.random() < epsilon:
            action = self.env.action_space.sample()
        # íƒí—˜ ê°ì†Œ ì‹œì‘ í”„ë ˆì„ ì´í›„ì—ëŠ” íƒí—˜ ê°ì†Œ ìµœì¢… í”„ë ˆì„ê¹Œì§€ íƒí—˜ ê°ì†Œ
        else:
            # í˜„ì¬ ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜í•œë‹¤.
            state_v = torch.as_tensor(self.state).to(device)
            state_v = state_v.unsqueeze(0)

            # í˜„ì¬ ìƒíƒœë¥¼ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼ ì‹œì¼œì„œ valueë¥¼ ì¸¡ì •í•˜ê³  ìµœëŒ€ valueë¥¼ ê°€ì§€ëŠ” ì•¡ì…˜ì„ ì„ íƒí•œë‹¤.
            q_vals_v = net(state_v)
            _, act_v = torch.max(q_vals_v, dim=1)
            action = int(act_v.item())

        # ì„ íƒí•œ ì•¡ì…˜ì„ í™˜ê²½ì—ì„œ ì‹¤í–‰í•œë‹¤.
        new_state, reward, done, truncated, _ = self.env.step(action)
        self.total_reward += reward

        # ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì¶”ê°€í•œë‹¤.
        exp = Experience(
            state=self.state,
            action=action,
            reward=float(reward),
            done_or_trunc=done or truncated,
            next_state=new_state,
        )
        self.replay_buffer.append(exp)
        self.state = new_state

        # í™˜ê²½ì´ ì¢…ë£Œë˜ì—ˆìœ¼ë©´ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•œë‹¤.
        if done or truncated:
            done_reward = self.total_reward
            self._reset()

        # ì¢…ë£Œë˜ì—ˆìœ¼ë©´ ë³´ìƒì„ ë°˜í™˜í•œë‹¤.
        return done_reward

def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:
    '''
    ê²½í—˜ì„ í…ì„œë¡œ ë³€í™˜í•œë‹¤.
    '''
    # ì´ˆê¸°í™”
    states, actions, rewards, dones, next_states = [], [], [], [], []

    # ë°°ì—´ì— ì¶”ê°€
    for e in batch:
        states.append(e.state)
        actions.append(e.action)
        rewards.append(e.reward)
        dones.append(e.done_or_trunc)
        next_states.append(e.next_state)

    # í…ì„œë¡œ ë³€í™˜
    states_t = torch.as_tensor(np.asarray(states))
    actions_t = torch.LongTensor(actions)
    rewards_t = torch.FloatTensor(rewards)
    dones_t = torch.BoolTensor(dones)
    next_states_t = torch.as_tensor(np.asarray(next_states))
    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \
        dones_t.to(device),  next_states_t.to(device)

def calc_loss(
        batch: tt.List[Experience],
        net: dqn_model.DQN,
        tgt_net: dqn_model.DQN,
        device: torch.device,
        ) -> torch.Tensor:
    '''
    ì†ì‹¤ì„ ê³„ì‚°í•œë‹¤.
    '''
    # í…ì„œë¡œ ë³€í™˜
    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)

    # í˜„ì¬ ìƒíƒœì—ì„œ ìµœì ì˜ ì•¡ì…˜ì„ ì„ íƒí•œë‹¤.
    state_action_values = net(states_t).gather(
        1, actions_t.unsqueeze(-1)
    ).squeeze(-1)

    # ë‹¤ìŒ ìƒíƒœì—ì„œ ìµœì ì˜ ì•¡ì…˜ì„ ì„ íƒí•œë‹¤.
    with torch.no_grad():
        next_state_values = tgt_net(new_states_t).max(1)[0]
        next_state_values[dones_t] = 0.0
        next_state_values = next_state_values.detach()

    # ê¸°ëŒ€ ìƒíƒœ-ì•¡ì…˜ ê°’ì„ ê³„ì‚°í•œë‹¤.
    expected_state_action_values = next_state_values * GAMMA + rewards_t

    # ì†ì‹¤ì„ ê³„ì‚°í•œë‹¤.
    return nn.MSELoss()(state_action_values, expected_state_action_values)

'''
Q(s,a)ì™€ íƒ€ê¹ƒ QÌ‚(s,a)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”
ì´ˆê¸° íƒí—˜ í™•ë¥  ğœ– â† 1.0
ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”

í™•ë¥  ğœ–ìœ¼ë¡œ ë¬´ì‘ìœ„ í–‰ë™ a ì„ íƒ,
ì•„ë‹ˆë¼ë©´ a = argmaxâ‚ Q(s,a)

aë¥¼ ì‹¤í–‰í•˜ê³  ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€² ê´€ì¸¡

(s, a, r, sâ€²) íŠœí”Œì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì €ì¥

ë¦¬í”Œë ˆì´ ë²„í¼ì—ì„œ ë¬´ì‘ìœ„ ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§

ê° ìƒ˜í”Œì— ëŒ€í•´ íƒ€ê¹ƒ ê°’ y ê³„ì‚°
(ì˜ˆ: y = r + Î³Â·maxâ‚â€² QÌ‚(sâ€², aâ€²))

ì†ì‹¤ ê³„ì‚°:
â„’ = (Q(s,a) âˆ’ y)Â²

ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ Q ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
â†’ SGD ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©

ë§¤ NìŠ¤í…ë§ˆë‹¤, Qì˜ ê°€ì¤‘ì¹˜ë¥¼ QÌ‚ë¡œ ë³µì‚¬

ìˆ˜ë ´í•  ë•Œê¹Œì§€ 2ë²ˆë¶€í„° ë°˜ë³µ
'''

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dev", default="cpu", help="Device name, default=cpu")
    parser.add_argument("--env", default=DEFAULT_ENV_NAME,
                        help="Name of the environment, default=" + DEFAULT_ENV_NAME)
    args = parser.parse_args()
    device = torch.device(args.dev)

    # í™˜ê²½ ìƒì„±
    env = wrappers.make_env(args.env)

    # ë„¤íŠ¸ì›Œí¬ ìƒì„±
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    writer = SummaryWriter(comment="-" + args.env)
    print("Network:", net)

    # ë¦¬í”Œë ˆì´ ë²„í¼ ìƒì„±
    buffer = ReplayBuffer(REPLAY_SIZE)
    agent = Agent(env, buffer)
    epsilon = EPSILON_START

    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)
    total_rewards = []
    frame_idx = 0
    ts_frame = 0
    ts = time.time()
    best_m_reward = None

    # í•™ìŠµ ì‹œì‘
    while True:
        frame_idx += 1
        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)

        reward = agent.play_step(net, device, epsilon)
        if reward is not None:
            total_rewards.append(reward)
            speed = (frame_idx - ts_frame) / (time.time() - ts)
            ts_frame = frame_idx
            ts = time.time()
            m_reward = np.mean(total_rewards[-100:])
            print(f"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, "
                  f"eps {epsilon:.2f}, speed {speed:.2f} f/s")
            writer.add_scalar("epsilon", epsilon, frame_idx)
            writer.add_scalar("speed", speed, frame_idx)
            writer.add_scalar("reward_100", m_reward, frame_idx)
            writer.add_scalar("reward", reward, frame_idx)
            if best_m_reward is None or best_m_reward < m_reward:
                torch.save(net.state_dict(), args.env + "-best_%.0f.dat" % m_reward)
                if best_m_reward is not None:
                    print(f"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}")
                best_m_reward = m_reward
            if m_reward > MEAN_REWARD_BOUND:
                print("Solved in %d frames!" % frame_idx)
                break
        if len(buffer) < REPLAY_START_SIZE:
            continue
        if frame_idx % SYNC_TARGET_FRAMES == 0:
            tgt_net.load_state_dict(net.state_dict())

        optimizer.zero_grad()
        batch = buffer.sample(BATCH_SIZE)
        loss_t = calc_loss(batch, net, tgt_net, device)
        loss_t.backward()
        optimizer.step()
    writer.close()